
help:  ## Show help
	@grep -E '^[.a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[36m%-30s\033[0m %s\n", $$1, $$2}'

clean: ## Clean autogenerated files
	rm -rf dist
	find . -type f -name "*.DS_Store" -ls -delete
	find . | grep -E "(__pycache__|\.pyc|\.pyo)" | xargs rm -rf
	find . | grep -E ".pytest_cache" | xargs rm -rf
	find . | grep -E ".ipynb_checkpoints" | xargs rm -rf
	rm -f .coverage

clean-logs: ## Clean logs
	rm -rf logs/**

format: ## Run pre-commit hooks
	pre-commit run -a

sync: ## Merge changes from main branch to your current branch
	git pull
	git pull origin main

test: ## Run not slow tests
	pytest -k "not slow"

test-full: ## Run all tests
	pytest

train: ## Train the model
	python src/train.py


prepare_data:
	python scripts/download_paths.py
	python scripts/render_data.py
	python scripts/get_vlm_embeddings.py +vlm='clip'
	python scripts/generate_splits.py

overfit_all:
	python scripts/train.py experiment=overfit/object logger=wandb
	python scripts/train.py experiment=overfit/contrastive logger=wandb
	python scripts/train.py experiment=overfit/autoencoder logger=wandb

batch_all:
	python scripts/train.py experiment=batch/object logger=wandb
	python scripts/train.py experiment=batch/contrastive logger=wandb
	python scripts/train.py experiment=batch/autoencoder logger=wandb

train_all:
	python scripts/train.py experiment=train/object logger=wandb
	python scripts/train.py experiment=train/contrastive logger=wandb
	python scripts/train.py experiment=train/autoencoder logger=wandb

train_object:
	python scripts/train.py experiment=overfit/object logger=wandb
	python scripts/train.py experiment=batch/object logger=wandb
	python scripts/train.py experiment=train/object logger=wandb

train_autoencoder:
	python scripts/train.py experiment=overfit/autoencoder logger=wandb
	python scripts/train.py experiment=batch/autoencoder logger=wandb
	python scripts/train.py experiment=train/autoencoder logger=wandb

train_now:
	# python scripts/train.py experiment=train/autoencoder logger=wandb
	# python scripts/train.py experiment=train/autoencoder logger=wandb model.loss.weight_similarity=1
	python scripts/train.py experiment=train/autoencoder logger=wandb model.loss.weight_autoencoder=0.2
	python scripts/train.py experiment=train/autoencoder logger=wandb model.loss.weight_autoencoder=0.8
	python srcipts/train.py experiment=train/autoencoder logger=wandb model.net.dropout_rate=0.2
	python srcipts/train.py experiment=train/autoencoder logger=wandb model.net.dropout_rate=0.2 model.loss.weight_similarity=1