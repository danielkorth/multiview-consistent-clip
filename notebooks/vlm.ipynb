{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\")\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n",
    "\n",
    "# img = plt.imread(\"/root/multiview-robust-clip/data/objaverse/renderings/0a0c75dab0844e7fa5b299d4af858bec/004.png\")[:,:, :3]\n",
    "\n",
    "# texts = [\"a photo of 2 cats\", \"a photo of 2 dogs\"]\n",
    "\n",
    "# inputs = processor(text=texts, images=img, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer, AutoModel, AutoProcessor, AutoTokenizer\n",
    "\n",
    "class VLM(nn.Module):\n",
    "    def __init__(self, vlm_name: str = 'clip'):\n",
    "        super().__init__()\n",
    "        if vlm_name == 'clip':\n",
    "            self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "            self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "            self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        elif vlm_name == 'siglip':\n",
    "            self.model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\")\n",
    "            self.processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"google/siglip-base-patch16-224\")\n",
    "        else:\n",
    "            raise Exception(\"Provide a valid VLM name [clip | siglip]\")\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def embed_image(self, x):\n",
    "        inputs = self.processor(images=x, return_tensors=\"pt\")\n",
    "        return self.model.get_image_features(**inputs)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_text(self, x: List[str]):\n",
    "        tokens = self.tokenizer(x, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return self.model.get_text_features(**tokens)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def embed(self, texts: List[str], image):\n",
    "        inputs = self.processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        outputs = self.model(**inputs)\n",
    "        return {\n",
    "            \"text_embed\": outputs['text_embeds'],\n",
    "            \"image_embed\": outputs['image_embeds']\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.vlm import VLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = VLM('clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siglip = VLM('sigplip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread(\"/root/multiview-robust-clip/data/objaverse/renderings/0a0c75dab0844e7fa5b299d4af858bec/004.png\")[:,:, :3]\n",
    "\n",
    "texts = [\"a photo of 2 cats\", \"a photo of 2 dogs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siglip.embed_image(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siglip.embed(texts, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.forward_text(texts).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['text_embeds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.get_image_features(inputs['pixel_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.image_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"/root/home/\") / \"image.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the JSON file containing object names\n",
    "with open('/root/multiview-robust-clip/data/objaverse/uid_to_name.json', 'r') as file:\n",
    "    uid_to_name = json.load(file)\n",
    "\n",
    "# Get all unique IDs from the uid_to_name dictionary\n",
    "all_uids = list(uid_to_name.keys())\n",
    "\n",
    "# Shuffle the list of UIDs to ensure randomness\n",
    "np.random.shuffle(all_uids)\n",
    "\n",
    "# Calculate the number of samples for each split\n",
    "num_total = len(all_uids)\n",
    "num_train = int(num_total * 0.8)\n",
    "num_val = int(num_total * 0.1)\n",
    "num_test = num_total - num_train - num_val\n",
    "\n",
    "# Split the UIDs into train, validation, and test sets\n",
    "train_uids = all_uids[:num_train]\n",
    "val_uids = all_uids[num_train:num_train + num_val]\n",
    "test_uids = all_uids[num_train + num_val:]\n",
    "\n",
    "# Create DataFrames for train, validation, and test sets\n",
    "train_df = pd.DataFrame(train_uids, columns=['uid'])\n",
    "val_df = pd.DataFrame(val_uids, columns=['uid'])\n",
    "test_df = pd.DataFrame(test_uids, columns=['uid'])\n",
    "\n",
    "# Save DataFrames to CSV files\n",
    "train_df.to_csv('/root/multiview-robust-clip/data/objaverse/train.csv', index=False)\n",
    "val_df.to_csv('/root/multiview-robust-clip/data/objaverse/val.csv', index=False)\n",
    "test_df.to_csv('/root/multiview-robust-clip/data/objaverse/test.csv', index=False)\n",
    "\n",
    "# Create a DataFrame for overfitting with only a single ID\n",
    "train_overfit_df = pd.DataFrame(train_uids[:1], columns=['uid'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "train_overfit_df.to_csv('/root/multiview-robust-clip/data/objaverse/train_overfit.csv', index=False)\n",
    "\n",
    "# Create a DataFrame for a small batch training with 8 IDs\n",
    "train_batch_df = pd.DataFrame(train_uids[:8], columns=['uid'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "train_batch_df.to_csv('/root/multiview-robust-clip/data/objaverse/train_batch.csv', index=False)\n",
    "\n",
    "print(\"CSV files for overfitting and small batch training have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvr_clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
